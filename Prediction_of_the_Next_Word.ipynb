{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjcR00pSogchwbZw5hQdr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kshitez-Pratap-Singh/Next-Word-Prediction/blob/main/Prediction_of_the_Next_Word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEXT WORD PREDICTION**"
      ],
      "metadata": {
        "id": "mc4O_dr9VdMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Description**  \n",
        "Identifying the most likely word to follow a given string of words is the basic goal of the Natural Language Processing (NLP) task of “next word prediction.” This predictive skill is essential in various applications, including text auto-completion, speech recognition, and machine translation. Deep learning approaches have transformed NLP by attaining remarkable success in various language-related tasks, such as next-word prediction.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=14jlJ-bdtHoSl0fMI-HCfT2QvNU-hC1Fc)"
      ],
      "metadata": {
        "id": "XMQhkQRwVpSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing the Essential Libraries**"
      ],
      "metadata": {
        "id": "sUxI3dtJWR6-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klyF3jebElTe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Uploading the File from System**"
      ],
      "metadata": {
        "id": "wP0_RcJnWXc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "tkSXL69KYiEi",
        "outputId": "3af99a19-ae5c-4f9d-d679-e362e75fb2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dc94ff6b-3910-46f9-9e14-8379bba906f2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dc94ff6b-3910-46f9-9e14-8379bba906f2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Romeo and Juliet.txt to Romeo and Juliet.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing the Uploaded File**"
      ],
      "metadata": {
        "id": "uf64oaiEWfSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file=open(\"Romeo and Juliet.txt\",'r',encoding='utf8')\n",
        "lines=[]\n",
        "for i in file:\n",
        "  lines.append(i)\n",
        "data=\"\"\n",
        "for i in lines:\n",
        "  data=' '.join(lines)\n",
        "\n",
        "data=data.replace('\\n','').replace('\\r','').replace('ufeff','').replace('\"','').replace('\"','')\n",
        "data=data.split()\n",
        "data=' '.join(data)\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "-g6um5B5Y0eO",
        "outputId": "9af890a5-b206-4f46-e8ae-444b4f6b4a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Project Gutenberg eBook of Romeo and Juliet This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Romeo and Juliet Author: William Shakespeare Release date: November 1, 1998 [eBook #1513] Most recently updated: June 27, 2023 Language: English *** START OF THE PROJECT GUTENBERG EBOOK ROMEO AND JULIET *** THE TRAGEDY OF ROMEO AND JULIET by William Shakespeare Contents THE PROLOGUE. ACT I Scene I. A public place. Scene II. A Street. Scene III. Room in Capulet’s House. Scene IV. A Street. Scene V. A Hall in Capulet’s House. ACT II CHORUS. Scene I. An open place adjoining Capulet’s Garden. Scene II'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization of Word**"
      ],
      "metadata": {
        "id": "W-QC1Bn_WmGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "pickle.dump(tokenizer,open('token.pkl','wb'))\n",
        "sequence_data=tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4W-TzUraVAr",
        "outputId": "7ad0a827-a7fa-4f60-c6c2-4bfdbf02238d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 54, 129, 306, 6, 12, 2, 22, 16, 306, 8, 18, 1, 150, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE8MDcm9bAUH",
        "outputId": "3bc23511-f087-4443-9a35-c46ccabb3d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29285"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Vocabulary Creation**"
      ],
      "metadata": {
        "id": "jbyY8GiCWs9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(tokenizer.word_index)+1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlJYKcHdbEGp",
        "outputId": "3983e574-9ad1-4895-ae01-9381c8ed532f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences=[]\n",
        "for i in range(3,len(sequence_data)):\n",
        "  words=sequence_data[i-3:i+1]\n",
        "  sequences.append(words)\n",
        "sequence=np.array(sequences)\n",
        "sequences[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cScbH5EbOO0",
        "outputId": "600c883b-e3cf-4d99-a2b6-cb43037f46c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 54, 129, 306],\n",
              " [54, 129, 306, 6],\n",
              " [129, 306, 6, 12],\n",
              " [306, 6, 12, 2],\n",
              " [6, 12, 2, 22],\n",
              " [12, 2, 22, 16],\n",
              " [2, 22, 16, 306],\n",
              " [22, 16, 306, 8],\n",
              " [16, 306, 8, 18],\n",
              " [306, 8, 18, 1],\n",
              " [8, 18, 1, 150],\n",
              " [18, 1, 150, 6],\n",
              " [1, 150, 6, 653],\n",
              " [150, 6, 653, 969],\n",
              " [6, 653, 969, 7]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting the Dataset in Dependent and Independent**"
      ],
      "metadata": {
        "id": "xZwRbRqGWzly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=[]\n",
        "y=[]\n",
        "for i in sequences:\n",
        "  x.append(i[0:3])\n",
        "  y.append(i[3])\n",
        "\n",
        "x=np.array(x)\n",
        "y=np.array(y)"
      ],
      "metadata": {
        "id": "mp5fWZV3bujE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Data: ',x)\n",
        "print('Response: ',y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atP72h6gcAbf",
        "outputId": "63419fac-d1e5-443a-b467-44ec2784a8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  [[   1   54  129]\n",
            " [  54  129  306]\n",
            " [ 129  306    6]\n",
            " ...\n",
            " [4295    3  183]\n",
            " [   3  183  226]\n",
            " [ 183  226  234]]\n",
            "Response:  [306   6  12 ... 226 234 564]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=to_categorical(y,num_classes=vocab_size)\n",
        "y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OJWWMfxcJd_",
        "outputId": "0c68921c-8877-408f-94ed-86e69ea7d802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating the Model**"
      ],
      "metadata": {
        "id": "f8_TLTODW_7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,10,input_length=3))\n",
        "model.add(LSTM(1000,return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000,activation='relu'))\n",
        "model.add(Dense(vocab_size,activation='softmax'))"
      ],
      "metadata": {
        "id": "LWKJtE6mccZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yJ3F1rMdAOo",
        "outputId": "99283dab-28b5-4a64-cece-096f54d5ca28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             42960     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4296)              4300296   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17392256 (66.35 MB)\n",
            "Trainable params: 17392256 (66.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint=ModelCheckpoint('next_words.h5',monitor='loss',verbose=1,save_best_only=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.001))\n",
        "model.fit(x,y,epochs=200,batch_size=64,callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F498WjprdDKj",
        "outputId": "bd084818-2ccf-43a2-9e40-5cf1096e9868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1704\n",
            "Epoch 1: loss improved from inf to 0.17040, saving model to next_words.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r458/458 [==============================] - 17s 28ms/step - loss: 0.1704\n",
            "Epoch 2/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1606\n",
            "Epoch 2: loss improved from 0.17040 to 0.16060, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 16ms/step - loss: 0.1606\n",
            "Epoch 3/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1515\n",
            "Epoch 3: loss improved from 0.16060 to 0.15155, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 16ms/step - loss: 0.1515\n",
            "Epoch 4/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1536\n",
            "Epoch 4: loss did not improve from 0.15155\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1536\n",
            "Epoch 5/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1661\n",
            "Epoch 5: loss did not improve from 0.15155\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1661\n",
            "Epoch 6/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1658\n",
            "Epoch 6: loss did not improve from 0.15155\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1658\n",
            "Epoch 7/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1490\n",
            "Epoch 7: loss improved from 0.15155 to 0.14897, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 16ms/step - loss: 0.1490\n",
            "Epoch 8/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1434\n",
            "Epoch 8: loss improved from 0.14897 to 0.14340, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1434\n",
            "Epoch 9/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1418\n",
            "Epoch 9: loss improved from 0.14340 to 0.14183, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 16ms/step - loss: 0.1418\n",
            "Epoch 10/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1381\n",
            "Epoch 10: loss improved from 0.14183 to 0.13812, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1381\n",
            "Epoch 11/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1353\n",
            "Epoch 11: loss improved from 0.13812 to 0.13529, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 16ms/step - loss: 0.1353\n",
            "Epoch 12/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1425\n",
            "Epoch 12: loss did not improve from 0.13529\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1425\n",
            "Epoch 13/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1520\n",
            "Epoch 13: loss did not improve from 0.13529\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1522\n",
            "Epoch 14/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1518\n",
            "Epoch 14: loss did not improve from 0.13529\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1518\n",
            "Epoch 15/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1548\n",
            "Epoch 15: loss did not improve from 0.13529\n",
            "458/458 [==============================] - 7s 16ms/step - loss: 0.1548\n",
            "Epoch 16/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1439\n",
            "Epoch 16: loss did not improve from 0.13529\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1441\n",
            "Epoch 17/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1350\n",
            "Epoch 17: loss improved from 0.13529 to 0.13527, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1353\n",
            "Epoch 18/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1249\n",
            "Epoch 18: loss improved from 0.13527 to 0.12486, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1249\n",
            "Epoch 19/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1258\n",
            "Epoch 19: loss did not improve from 0.12486\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1260\n",
            "Epoch 20/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1330\n",
            "Epoch 20: loss did not improve from 0.12486\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1329\n",
            "Epoch 21/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1385\n",
            "Epoch 21: loss did not improve from 0.12486\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1388\n",
            "Epoch 22/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1579\n",
            "Epoch 22: loss did not improve from 0.12486\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1581\n",
            "Epoch 23/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1438\n",
            "Epoch 23: loss did not improve from 0.12486\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1438\n",
            "Epoch 24/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1351\n",
            "Epoch 24: loss did not improve from 0.12486\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1351\n",
            "Epoch 25/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1379\n",
            "Epoch 25: loss did not improve from 0.12486\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1379\n",
            "Epoch 26/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1297\n",
            "Epoch 26: loss did not improve from 0.12486\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1295\n",
            "Epoch 27/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1297\n",
            "Epoch 27: loss did not improve from 0.12486\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1297\n",
            "Epoch 28/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1237\n",
            "Epoch 28: loss improved from 0.12486 to 0.12368, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1237\n",
            "Epoch 29/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1177\n",
            "Epoch 29: loss improved from 0.12368 to 0.11763, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1176\n",
            "Epoch 30/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1207\n",
            "Epoch 30: loss did not improve from 0.11763\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1207\n",
            "Epoch 31/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1409\n",
            "Epoch 31: loss did not improve from 0.11763\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1409\n",
            "Epoch 32/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1558\n",
            "Epoch 32: loss did not improve from 0.11763\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1558\n",
            "Epoch 33/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1331\n",
            "Epoch 33: loss did not improve from 0.11763\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1331\n",
            "Epoch 34/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1213\n",
            "Epoch 34: loss did not improve from 0.11763\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1214\n",
            "Epoch 35/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1146\n",
            "Epoch 35: loss improved from 0.11763 to 0.11477, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1148\n",
            "Epoch 36/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1141\n",
            "Epoch 36: loss improved from 0.11477 to 0.11406, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1141\n",
            "Epoch 37/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1149\n",
            "Epoch 37: loss did not improve from 0.11406\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1149\n",
            "Epoch 38/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1164\n",
            "Epoch 38: loss did not improve from 0.11406\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1164\n",
            "Epoch 39/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1397\n",
            "Epoch 39: loss did not improve from 0.11406\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1397\n",
            "Epoch 40/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1718\n",
            "Epoch 40: loss did not improve from 0.11406\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1718\n",
            "Epoch 41/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1410\n",
            "Epoch 41: loss did not improve from 0.11406\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1410\n",
            "Epoch 42/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1163\n",
            "Epoch 42: loss did not improve from 0.11406\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1163\n",
            "Epoch 43/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1102\n",
            "Epoch 43: loss improved from 0.11406 to 0.11024, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1102\n",
            "Epoch 44/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1075\n",
            "Epoch 44: loss improved from 0.11024 to 0.10749, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1075\n",
            "Epoch 45/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1081\n",
            "Epoch 45: loss did not improve from 0.10749\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1082\n",
            "Epoch 46/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1165\n",
            "Epoch 46: loss did not improve from 0.10749\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1165\n",
            "Epoch 47/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1344\n",
            "Epoch 47: loss did not improve from 0.10749\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1346\n",
            "Epoch 48/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1475\n",
            "Epoch 48: loss did not improve from 0.10749\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1492\n",
            "Epoch 49/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1361\n",
            "Epoch 49: loss did not improve from 0.10749\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1361\n",
            "Epoch 50/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1212\n",
            "Epoch 50: loss did not improve from 0.10749\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1212\n",
            "Epoch 51/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1184\n",
            "Epoch 51: loss did not improve from 0.10749\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1186\n",
            "Epoch 52/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1079\n",
            "Epoch 52: loss did not improve from 0.10749\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1079\n",
            "Epoch 53/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0994\n",
            "Epoch 53: loss improved from 0.10749 to 0.09947, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0995\n",
            "Epoch 54/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0977\n",
            "Epoch 54: loss improved from 0.09947 to 0.09787, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0979\n",
            "Epoch 55/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0990\n",
            "Epoch 55: loss did not improve from 0.09787\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0990\n",
            "Epoch 56/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0993\n",
            "Epoch 56: loss did not improve from 0.09787\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0993\n",
            "Epoch 57/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1281\n",
            "Epoch 57: loss did not improve from 0.09787\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1283\n",
            "Epoch 58/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.2292\n",
            "Epoch 58: loss did not improve from 0.09787\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.2293\n",
            "Epoch 59/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1447\n",
            "Epoch 59: loss did not improve from 0.09787\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1445\n",
            "Epoch 60/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1032\n",
            "Epoch 60: loss did not improve from 0.09787\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1033\n",
            "Epoch 61/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0965\n",
            "Epoch 61: loss improved from 0.09787 to 0.09654, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0965\n",
            "Epoch 62/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0943\n",
            "Epoch 62: loss improved from 0.09654 to 0.09429, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 16ms/step - loss: 0.0943\n",
            "Epoch 63/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0944\n",
            "Epoch 63: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0944\n",
            "Epoch 64/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0945\n",
            "Epoch 64: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0945\n",
            "Epoch 65/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1099\n",
            "Epoch 65: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1098\n",
            "Epoch 66/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1824\n",
            "Epoch 66: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1824\n",
            "Epoch 67/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1475\n",
            "Epoch 67: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1475\n",
            "Epoch 68/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1188\n",
            "Epoch 68: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1188\n",
            "Epoch 69/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1039\n",
            "Epoch 69: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1038\n",
            "Epoch 70/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1002\n",
            "Epoch 70: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1003\n",
            "Epoch 71/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0953\n",
            "Epoch 71: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0953\n",
            "Epoch 72/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0977\n",
            "Epoch 72: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0975\n",
            "Epoch 73/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0985\n",
            "Epoch 73: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0985\n",
            "Epoch 74/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0954\n",
            "Epoch 74: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0957\n",
            "Epoch 75/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0954\n",
            "Epoch 75: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0954\n",
            "Epoch 76/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1020\n",
            "Epoch 76: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1020\n",
            "Epoch 77/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.2368\n",
            "Epoch 77: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.2369\n",
            "Epoch 78/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1596\n",
            "Epoch 78: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1596\n",
            "Epoch 79/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1070\n",
            "Epoch 79: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1069\n",
            "Epoch 80/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0952\n",
            "Epoch 80: loss did not improve from 0.09429\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0953\n",
            "Epoch 81/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0927\n",
            "Epoch 81: loss improved from 0.09429 to 0.09251, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0925\n",
            "Epoch 82/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0901\n",
            "Epoch 82: loss improved from 0.09251 to 0.09022, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0902\n",
            "Epoch 83/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0912\n",
            "Epoch 83: loss did not improve from 0.09022\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0913\n",
            "Epoch 84/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0931\n",
            "Epoch 84: loss did not improve from 0.09022\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0931\n",
            "Epoch 85/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1120\n",
            "Epoch 85: loss did not improve from 0.09022\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1122\n",
            "Epoch 86/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1908\n",
            "Epoch 86: loss did not improve from 0.09022\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1909\n",
            "Epoch 87/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1408\n",
            "Epoch 87: loss did not improve from 0.09022\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1410\n",
            "Epoch 88/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1016\n",
            "Epoch 88: loss did not improve from 0.09022\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1018\n",
            "Epoch 89/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0945\n",
            "Epoch 89: loss did not improve from 0.09022\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0946\n",
            "Epoch 90/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0901\n",
            "Epoch 90: loss did not improve from 0.09022\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0903\n",
            "Epoch 91/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0887\n",
            "Epoch 91: loss improved from 0.09022 to 0.08883, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0888\n",
            "Epoch 92/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0887\n",
            "Epoch 92: loss improved from 0.08883 to 0.08875, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0887\n",
            "Epoch 93/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0918\n",
            "Epoch 93: loss did not improve from 0.08875\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0918\n",
            "Epoch 94/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1296\n",
            "Epoch 94: loss did not improve from 0.08875\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1295\n",
            "Epoch 95/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1978\n",
            "Epoch 95: loss did not improve from 0.08875\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1981\n",
            "Epoch 96/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1218\n",
            "Epoch 96: loss did not improve from 0.08875\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1218\n",
            "Epoch 97/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0977\n",
            "Epoch 97: loss did not improve from 0.08875\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0979\n",
            "Epoch 98/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0883\n",
            "Epoch 98: loss improved from 0.08875 to 0.08847, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0885\n",
            "Epoch 99/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0866\n",
            "Epoch 99: loss improved from 0.08847 to 0.08663, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0866\n",
            "Epoch 100/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0871\n",
            "Epoch 100: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0871\n",
            "Epoch 101/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0881\n",
            "Epoch 101: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0881\n",
            "Epoch 102/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0890\n",
            "Epoch 102: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0890\n",
            "Epoch 103/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0915\n",
            "Epoch 103: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0915\n",
            "Epoch 104/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1722\n",
            "Epoch 104: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1730\n",
            "Epoch 105/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1999\n",
            "Epoch 105: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1999\n",
            "Epoch 106/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1172\n",
            "Epoch 106: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1171\n",
            "Epoch 107/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0932\n",
            "Epoch 107: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0932\n",
            "Epoch 108/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0879\n",
            "Epoch 108: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0880\n",
            "Epoch 109/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0870\n",
            "Epoch 109: loss did not improve from 0.08663\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0868\n",
            "Epoch 110/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0860\n",
            "Epoch 110: loss improved from 0.08663 to 0.08603, saving model to next_words.h5\n",
            "458/458 [==============================] - 12s 26ms/step - loss: 0.0860\n",
            "Epoch 111/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0863\n",
            "Epoch 111: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0864\n",
            "Epoch 112/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0870\n",
            "Epoch 112: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0870\n",
            "Epoch 113/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0901\n",
            "Epoch 113: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0903\n",
            "Epoch 114/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1008\n",
            "Epoch 114: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1013\n",
            "Epoch 115/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.2453\n",
            "Epoch 115: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.2452\n",
            "Epoch 116/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1409\n",
            "Epoch 116: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1407\n",
            "Epoch 117/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0989\n",
            "Epoch 117: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0989\n",
            "Epoch 118/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0894\n",
            "Epoch 118: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0894\n",
            "Epoch 119/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0864\n",
            "Epoch 119: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0864\n",
            "Epoch 120/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0862\n",
            "Epoch 120: loss did not improve from 0.08603\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0862\n",
            "Epoch 121/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0855\n",
            "Epoch 121: loss improved from 0.08603 to 0.08545, saving model to next_words.h5\n",
            "458/458 [==============================] - 12s 26ms/step - loss: 0.0855\n",
            "Epoch 122/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0865\n",
            "Epoch 122: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0864\n",
            "Epoch 123/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0867\n",
            "Epoch 123: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0869\n",
            "Epoch 124/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0902\n",
            "Epoch 124: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0902\n",
            "Epoch 125/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1657\n",
            "Epoch 125: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1663\n",
            "Epoch 126/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1917\n",
            "Epoch 126: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1917\n",
            "Epoch 127/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1131\n",
            "Epoch 127: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1132\n",
            "Epoch 128/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0930\n",
            "Epoch 128: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0929\n",
            "Epoch 129/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0875\n",
            "Epoch 129: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0875\n",
            "Epoch 130/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0864\n",
            "Epoch 130: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0865\n",
            "Epoch 131/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0854\n",
            "Epoch 131: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0857\n",
            "Epoch 132/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0864\n",
            "Epoch 132: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0863\n",
            "Epoch 133/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0862\n",
            "Epoch 133: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0862\n",
            "Epoch 134/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0865\n",
            "Epoch 134: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0866\n",
            "Epoch 135/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0873\n",
            "Epoch 135: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0875\n",
            "Epoch 136/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1646\n",
            "Epoch 136: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1649\n",
            "Epoch 137/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.2087\n",
            "Epoch 137: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.2090\n",
            "Epoch 138/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1116\n",
            "Epoch 138: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1115\n",
            "Epoch 139/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0932\n",
            "Epoch 139: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0932\n",
            "Epoch 140/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0891\n",
            "Epoch 140: loss did not improve from 0.08545\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0890\n",
            "Epoch 141/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0850\n",
            "Epoch 141: loss improved from 0.08545 to 0.08511, saving model to next_words.h5\n",
            "458/458 [==============================] - 9s 19ms/step - loss: 0.0851\n",
            "Epoch 142/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0845\n",
            "Epoch 142: loss improved from 0.08511 to 0.08469, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0847\n",
            "Epoch 143/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0842\n",
            "Epoch 143: loss improved from 0.08469 to 0.08407, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 16ms/step - loss: 0.0841\n",
            "Epoch 144/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0838\n",
            "Epoch 144: loss improved from 0.08407 to 0.08385, saving model to next_words.h5\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0838\n",
            "Epoch 145/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0846\n",
            "Epoch 145: loss did not improve from 0.08385\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0845\n",
            "Epoch 146/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0876\n",
            "Epoch 146: loss did not improve from 0.08385\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0875\n",
            "Epoch 147/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1235\n",
            "Epoch 147: loss did not improve from 0.08385\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1242\n",
            "Epoch 148/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.2234\n",
            "Epoch 148: loss did not improve from 0.08385\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.2230\n",
            "Epoch 149/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1141\n",
            "Epoch 149: loss did not improve from 0.08385\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1140\n",
            "Epoch 150/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0975\n",
            "Epoch 150: loss did not improve from 0.08385\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0975\n",
            "Epoch 151/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0887\n",
            "Epoch 151: loss did not improve from 0.08385\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0887\n",
            "Epoch 152/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0849\n",
            "Epoch 152: loss did not improve from 0.08385\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0848\n",
            "Epoch 153/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0830\n",
            "Epoch 153: loss improved from 0.08385 to 0.08299, saving model to next_words.h5\n",
            "458/458 [==============================] - 10s 21ms/step - loss: 0.0830\n",
            "Epoch 154/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0831\n",
            "Epoch 154: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0830\n",
            "Epoch 155/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0834\n",
            "Epoch 155: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0833\n",
            "Epoch 156/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0839\n",
            "Epoch 156: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0840\n",
            "Epoch 157/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0864\n",
            "Epoch 157: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0864\n",
            "Epoch 158/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1572\n",
            "Epoch 158: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1572\n",
            "Epoch 159/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1994\n",
            "Epoch 159: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.1995\n",
            "Epoch 160/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1060\n",
            "Epoch 160: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1060\n",
            "Epoch 161/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0892\n",
            "Epoch 161: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0892\n",
            "Epoch 162/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0836\n",
            "Epoch 162: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0838\n",
            "Epoch 163/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0834\n",
            "Epoch 163: loss did not improve from 0.08299\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0836\n",
            "Epoch 164/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0823\n",
            "Epoch 164: loss improved from 0.08299 to 0.08230, saving model to next_words.h5\n",
            "458/458 [==============================] - 12s 27ms/step - loss: 0.0823\n",
            "Epoch 165/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0828\n",
            "Epoch 165: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0829\n",
            "Epoch 166/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0832\n",
            "Epoch 166: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0832\n",
            "Epoch 167/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0849\n",
            "Epoch 167: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0850\n",
            "Epoch 168/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1221\n",
            "Epoch 168: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1221\n",
            "Epoch 169/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.2058\n",
            "Epoch 169: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.2056\n",
            "Epoch 170/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1173\n",
            "Epoch 170: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1172\n",
            "Epoch 171/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0921\n",
            "Epoch 171: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0922\n",
            "Epoch 172/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0869\n",
            "Epoch 172: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0871\n",
            "Epoch 173/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0873\n",
            "Epoch 173: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0872\n",
            "Epoch 174/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0832\n",
            "Epoch 174: loss did not improve from 0.08230\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0831\n",
            "Epoch 175/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0820\n",
            "Epoch 175: loss improved from 0.08230 to 0.08205, saving model to next_words.h5\n",
            "458/458 [==============================] - 12s 26ms/step - loss: 0.0821\n",
            "Epoch 176/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0821\n",
            "Epoch 176: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0821\n",
            "Epoch 177/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0828\n",
            "Epoch 177: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0828\n",
            "Epoch 178/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0847\n",
            "Epoch 178: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0847\n",
            "Epoch 179/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0940\n",
            "Epoch 179: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0941\n",
            "Epoch 180/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.2241\n",
            "Epoch 180: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.2244\n",
            "Epoch 181/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1330\n",
            "Epoch 181: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1330\n",
            "Epoch 182/200\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0940\n",
            "Epoch 182: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0939\n",
            "Epoch 183/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0852\n",
            "Epoch 183: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0852\n",
            "Epoch 184/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0866\n",
            "Epoch 184: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0866\n",
            "Epoch 185/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.0887\n",
            "Epoch 185: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0887\n",
            "Epoch 186/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0907\n",
            "Epoch 186: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 6s 14ms/step - loss: 0.0907\n",
            "Epoch 187/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0947\n",
            "Epoch 187: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0947\n",
            "Epoch 188/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1094\n",
            "Epoch 188: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1094\n",
            "Epoch 189/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1174\n",
            "Epoch 189: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1174\n",
            "Epoch 190/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.1101\n",
            "Epoch 190: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1100\n",
            "Epoch 191/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0940\n",
            "Epoch 191: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0939\n",
            "Epoch 192/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0879\n",
            "Epoch 192: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0878\n",
            "Epoch 193/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0866\n",
            "Epoch 193: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0869\n",
            "Epoch 194/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0905\n",
            "Epoch 194: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0907\n",
            "Epoch 195/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1100\n",
            "Epoch 195: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1100\n",
            "Epoch 196/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1132\n",
            "Epoch 196: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1132\n",
            "Epoch 197/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1162\n",
            "Epoch 197: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.1164\n",
            "Epoch 198/200\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.1026\n",
            "Epoch 198: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.1026\n",
            "Epoch 199/200\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0941\n",
            "Epoch 199: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 14ms/step - loss: 0.0943\n",
            "Epoch 200/200\n",
            "457/458 [============================>.] - ETA: 0s - loss: 0.0878\n",
            "Epoch 200: loss did not improve from 0.08205\n",
            "458/458 [==============================] - 7s 15ms/step - loss: 0.0878\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e60175eea70>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Accuracy**"
      ],
      "metadata": {
        "id": "dnHtNmN2XH3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score=model.evaluate(x,y)\n",
        "print('Accuracy: ',score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaEG2Ipvg1YH",
        "outputId": "4ab40d9e-1dec-41d8-e9af-89967657154a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "916/916 [==============================] - 6s 6ms/step - loss: 0.0750\n",
            "Accuracy:  0.07498759776353836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model=load_model('next_words.h5')\n",
        "tokenizer=pickle.load(open('token.pkl','rb'))\n",
        "def Predict_Next_Word(model,tokenizer,text):\n",
        "  sequence=tokenizer.texts_to_sequences([text])\n",
        "  sequence=np.array(sequence)\n",
        "  preds=np.argmax(model.predict(sequence))\n",
        "  predicted_word=\"\"\n",
        "\n",
        "  for key,value in tokenizer.word_index.items():\n",
        "    if value==preds:\n",
        "      predicted_word=key\n",
        "      break\n",
        "\n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ],
      "metadata": {
        "id": "T9HFMWNqdiww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prediction**"
      ],
      "metadata": {
        "id": "aZAYVN0WXSRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  text=input('Enter your lines: ')\n",
        "\n",
        "  if text=='0':\n",
        "    print('Execution Completed...')\n",
        "    break\n",
        "\n",
        "  else:\n",
        "    try:\n",
        "      text=text.split(\" \")\n",
        "      text=text[-3:]\n",
        "      print(text)\n",
        "      Predict_Next_Word(model,tokenizer,text)\n",
        "\n",
        "    except Exception as e:\n",
        "      print('Error occured: ',e)\n",
        "      continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kGJBz3jgYL-",
        "outputId": "5d071bfb-a1bf-4064-b3fb-87bbe2d3dcf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your lines: how that thing\n",
            "['how', 'that', 'thing']\n",
            "1/1 [==============================] - 1s 609ms/step\n",
            "he\n",
            "Enter your lines: the day was\n",
            "['the', 'day', 'was']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "broke\n",
            "Enter your lines: and then he \n",
            "['then', 'he', '']\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "is\n",
            "Enter your lines: 0\n",
            "Execution Completed...\n"
          ]
        }
      ]
    }
  ]
}